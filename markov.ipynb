{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQDDX-JmcH_y",
        "outputId": "118f459d-4f3f-401d-cc5a-8d495551096a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected network: WMAN\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define network attributes and their weights (importance)\n",
        "attributes = ['bandwidth', 'price', 'cell_radius', 'security', 'power_consumption', 'traffic']\n",
        "attribute_weights = {'bandwidth': 0.2, 'price': 0.2, 'cell_radius': 0.1, 'security': 0.15, 'power_consumption': 0.15, 'traffic': 0.2}\n",
        "\n",
        "# Define user preferences for applications and additional preferences (money, quality, battery life, mobility)\n",
        "class User:\n",
        "    def __init__(self, application, preference1, preference2, preference3):\n",
        "        self.application = application\n",
        "        self.preference1 = preference1  # e.g., 'money', 'quality'\n",
        "        self.preference2 = preference2  # e.g., 'battery', 'mobility'\n",
        "        self.preference3 = preference3  # e.g., 'money', 'quality'\n",
        "\n",
        "# Define network class with attributes\n",
        "class Network:\n",
        "    def __init__(self, name, bandwidth, price, cell_radius, security, power_consumption, traffic):\n",
        "        self.name = name\n",
        "        self.bandwidth = bandwidth\n",
        "        self.price = price\n",
        "        self.cell_radius = cell_radius\n",
        "        self.security = security\n",
        "        self.power_consumption = power_consumption\n",
        "        self.traffic = traffic\n",
        "\n",
        "# Define a basic Markov decision process (MDP) framework for network selection\n",
        "class NetworkSelectionMDP:\n",
        "    def __init__(self, user, networks, actions):\n",
        "        self.user = user\n",
        "        self.networks = networks\n",
        "        self.actions = actions\n",
        "        self.state_space = [tuple(network_attributes) for network_attributes in self._generate_state_space()]\n",
        "        self.action_space = list(range(len(actions)))\n",
        "        self.q_table = np.zeros((len(self.state_space), len(self.action_space)))\n",
        "\n",
        "    def _generate_state_space(self):\n",
        "        # Generate all possible combinations of network attributes\n",
        "        state_space = []\n",
        "        for network in self.networks:\n",
        "            state = [getattr(network, attr) for attr in attributes]\n",
        "            state_space.append(state)\n",
        "        return state_space\n",
        "\n",
        "    def _calculate_utility(self, state_idx):\n",
        "        # Calculate utility based on the current state (network attributes)\n",
        "        state = self.state_space[state_idx]\n",
        "        utility = sum(state[i] * attribute_weights[attr] for i, attr in enumerate(attributes))\n",
        "\n",
        "        # Apply user preferences to adjust utility\n",
        "        if self.user.application == 'conversational':\n",
        "            if self.user.preference1 == 'money':\n",
        "                utility -= state[attributes.index('price')] * 0.2  # Decrease utility for higher prices\n",
        "        elif self.user.application == 'streaming':\n",
        "            if self.user.preference1 == 'quality':\n",
        "                utility += state[attributes.index('bandwidth')] * 0.2  # Increase utility for higher bandwidth\n",
        "        elif self.user.application == 'interactive':\n",
        "            if self.user.preference2 == 'battery':\n",
        "                utility -= state[attributes.index('power_consumption')] * 0.3  # Decrease utility for higher power consumption\n",
        "            elif self.user.preference2 == 'mobility':\n",
        "                utility += state[attributes.index('cell_radius')] * 0.3  # Increase utility for higher cell radius\n",
        "\n",
        "        return utility\n",
        "\n",
        "    def _transition(self, state_idx, action_idx):\n",
        "        # Transition to a new state based on the selected action (network selection)\n",
        "        next_state_idx = action_idx  # Assume direct transition to the selected network (action index)\n",
        "        return next_state_idx\n",
        "\n",
        "    def train(self, num_episodes=100, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
        "        # Train the Q-learning agent to learn the optimal policy\n",
        "        for _ in range(num_episodes):\n",
        "            state_idx = np.random.randint(0, len(self.state_space))  # Random initial state\n",
        "            for _ in range(len(self.networks)):  # Maximum steps = number of networks\n",
        "                if np.random.uniform(0, 1) < epsilon:\n",
        "                    action_idx = np.random.choice(self.action_space)  # Explore (random action)\n",
        "                else:\n",
        "                    action_idx = np.argmax(self.q_table[state_idx])  # Exploit (best action)\n",
        "\n",
        "                next_state_idx = self._transition(state_idx, action_idx)\n",
        "                reward = self._calculate_utility(next_state_idx)\n",
        "\n",
        "                # Update Q-value using Q-learning update rule\n",
        "                self.q_table[state_idx, action_idx] += learning_rate * (\n",
        "                    reward + discount_factor * np.max(self.q_table[next_state_idx]) - self.q_table[state_idx, action_idx])\n",
        "\n",
        "                state_idx = next_state_idx  # Transition to the next state\n",
        "\n",
        "    def select_network(self):\n",
        "        # Select the best network based on the learned Q-values (optimal policy)\n",
        "        state_idx = np.argmax(self.q_table[0])  # Start from the initial state (index 0)\n",
        "        best_network = self.networks[state_idx]\n",
        "        return best_network\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Create user with specified application and preferences\n",
        "    user = User(application='conversational', preference1='money', preference2='battery', preference3='quality')\n",
        "\n",
        "    # Create network instances with simulated attributes\n",
        "    networks = [\n",
        "                Network(\"WWAN\", bandwidth=2, price=50, cell_radius=2000, security=3, power_consumption=1/100, traffic=50),\n",
        "        Network(\"WMAN\", bandwidth=10, price=20, cell_radius=2000, security=3, power_consumption=1/100, traffic=70),\n",
        "        Network(\"WLAN\", bandwidth=54, price=5, cell_radius=75, security=1, power_consumption=1/50, traffic=90),\n",
        "        Network(\"WPAN\", bandwidth=1, price=1, cell_radius=10, security=2, power_consumption=1/1000, traffic=90)\n",
        "    ]\n",
        "\n",
        "    # Define network selection actions (networks to choose from)\n",
        "    actions = networks\n",
        "\n",
        "    # Create a NetworkSelectionMDP instance and train the Q-learning agent\n",
        "    mdp = NetworkSelectionMDP(user, networks, actions)\n",
        "    mdp.train(num_episodes=1000)\n",
        "\n",
        "    # Select the best network based on the learned policy\n",
        "    best_network = mdp.select_network()\n",
        "    print(f\"Selected network: {best_network.name}\")\n"
      ]
    }
  ]
}